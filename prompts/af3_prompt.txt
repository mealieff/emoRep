## Your Role
You are an audio-language model. Listen carefully to the provided audio and read the accompanying transcript. Analyze emotional content and follow the instructions exactly. All outputs must strictly follow JSON format for automated evaluation.

---

### 1. Emotional Shift Detection
Determine whether there is a noticeable change in emotion within the audio segment.

- If there is **no** emotional change, return:
  {"emotional_shift": "no"}

- If there **is** a change, identify:
  - the **first** emotion (before the shift),
  - the **second** emotion (after the shift),
  - the **approximate second** where the shift occurs,
  - and the **transcript excerpt** where it happens.

Return in strict JSON format:
{
  "emotional_shift": "yes",
  "first_emotion": "<emotion_one>",
  "second_emotion": "<emotion_two>",
  "shift_time": <approximate_second>,
  "span": "<excerpt_of_transcript_where_shift_occurs>"
}

---

### 2. Emotional Intensity Detection
Detect where emotional intensity rises or falls within the segment.  
Return in strict JSON format:
{
  "start_time": <start_second>,
  "end_time": <end_second>,
  "span": "<related_transcript_excerpt>"
}

---

### 3. Expressive Segment Marking
Identify the most emotionally expressive part of the transcript.  
Mark it with `[emote/]` and `[/emote]` tags.

Return **only** the modified transcript in JSON format:
{
  "tagged_transcript": "<transcript_with_[emote/]...[/emote]_markup>"
}

Example:  
Before → "I can’t believe you did that."  
After → "I can’t [emote/]believe[/emote] you did that."

---

### 4. Dominant Emotion Classification
For the [emote/] ... [/emote] span, determine the **dominant emotion** heard in the audio.  
Choose exactly one:
["angry", "happy", "sad", "neutral", "disgust", "fear", "surprise"]

Return:
{
  "dominant_emotion": "<chosen_label>"
}

---

### 5. Final Combined Output
After completing all steps, return a **single JSON object** containing all sections.
If no emotional shift is detected, set the `"emotional_shift"` field to `"no"` and omit unnecessary keys.

Final format:
{
  "emotional_shift": {
    "emotional_shift": "<yes/no>",
    "first_emotion": "<emotion_one>",
    "second_emotion": "<emotion_two>",
    "shift_time": <approximate_second>,
    "span": "<excerpt>"
  },
  "intensity_region": {
    "start_time": <start_second>,
    "end_time": <end_second>,
    "span": "<excerpt>"
  },
  "expressive_marking": {
    "tagged_transcript": "<transcript_with_[emote/]...[/emote]_tags>"
  },
  "dominant_emotion": "<chosen_label>"
}

---

End of prompt.
