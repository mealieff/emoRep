{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i9vVwhfMMVB"
      },
      "source": [
        "## Qwen tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf8k3OTMzEjn"
      },
      "source": [
        "this notebook (a) loads ~20 examples from one of the emotional-span datasets (IEMOCAP, MELD, SEMAINE, RECOLA), (b) queries Qwen-audio with prompts tailored for span-level detection, and (c) collects outputs into JSON.\n",
        "\n",
        "Second part: does this per speaker in a segment of MSP Podcast audio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tk28g0zsM4N6"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets soundfile accelerate opensmile librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-UuscQ1vNYCb"
      },
      "outputs": [],
      "source": [
        "#return to this when running on supercomputer\n",
        "\n",
        "import torch, json\n",
        "import soundfile as sf\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "model_id = \"Qwen/Qwen2-Audio-7B\"\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_4bit=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jweQy6CYM-rp"
      },
      "source": [
        "audio analysis: users could provide audio and text instructions for analysis during the interaction;\n",
        "\n",
        "Options: \"iemocap\", \"meld\", \"semaine\", \"recola\" (if available on HuggingFace Hub)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oWzWdK50LIq"
      },
      "outputs": [],
      "source": [
        "# Example: dataset_name = \"iemocap\"\n",
        "dataset_name = \"iemocap\"\n",
        "\n",
        "# Load first 20 examples\n",
        "dataset = load_dataset(dataset_name, split=\"train[:20]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBQdgN4A0eCG"
      },
      "outputs": [],
      "source": [
        "def run_qwen_analysis(audio, transcript, file_id):\n",
        "    inputs = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    def ask_qwen(prompt):\n",
        "        generated_ids = model.generate(**inputs, max_new_tokens=512, decoder_input_ids=processor.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device))\n",
        "        return processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    fast_half_prompt = (\n",
        "        \"Listen carefully to the second half of the audio (from the midpoint to the end). \"\n",
        "        \"Identify what emotions, tone, or expressive changes are most prominent. \"\n",
        "        \"Summarize briefly in 1–2 sentences.\"\n",
        "\n",
        "    prompt_bme = (\n",
        "        \"Summarize the emotional tone and content at the beginning, middle, and end of the audio. \"\n",
        "        \"Respond strictly in JSON with keys 'beginning', 'middle', and 'end', \"\n",
        "        \"where each value is a short description of what you hear emotionally and verbally.\"\n",
        "    )\n",
        "\n",
        "    prompt_shift = (\n",
        "        \"Locate the first clear emotional shift in the transcript (e.g., from neutral to angry, \"\n",
        "        \"calm to excited, or sad to hopeful). \"\n",
        "        \"Return a JSON object with fields: {'shift_time': approximate_second, 'span': transcript_excerpt}. \"\n",
        "        \"Keep 'span' to ≤20 words centered around the shift point.\"\n",
        "    )\n",
        "\n",
        "    prompt_intensity = (\n",
        "        \"Identify the time range where the speaker’s emotional intensity increases or decreases significantly. \"\n",
        "        \"Return a JSON object with fields: {'start_time': X, 'end_time': Y, 'span': transcript_excerpt}. \"\n",
        "        \"Choose a transcript span of ~1–2 sentences that best illustrates the change.\"\n",
        "    )\n",
        "\n",
        "    prompt_expressive = (\n",
        "        \"Find the portion of the transcript where the speaker is most emotionally expressive \"\n",
        "        \"(e.g., raised voice, strong affect, laughter, crying). \"\n",
        "        \"Mark this portion by wrapping it with [emote/] ... [/emote]. \"\n",
        "        \"Return only the modified transcript, with exactly one [emote/] ... [/emote] span.\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"file\": file_id,\n",
        "        \"transcript\": transcript,\n",
        "        \"fast_half\": ask_qwen(fast_half_prompt),\n",
        "        \"begin_mid_end\": ask_qwen(prompt_bme),\n",
        "        \"emotional_shift\": ask_qwen(prompt_shift),\n",
        "        \"intensity_span\": ask_qwen(prompt_intensity),\n",
        "        \"expressive_span\": ask_qwen(prompt_expressive),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNkYGzBV0lQ7"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "for i, ex in enumerate(dataset):\n",
        "    audio = ex[\"audio\"]\n",
        "    transcript = ex.get(\"transcription\") or ex.get(\"text\") or \"\"\n",
        "## un/comment result as needed\n",
        "    #result = run_mellow_analysis(audio, transcript, f\"sample_{i}\")\n",
        "    result = run_qwen_analysis(audio, transcript, f\"sample_{i}\")\n",
        "    results.append(result)\n",
        "\n",
        "with open(\"span_detection_results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"✅ Saved results to iemo_span_detection_results.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MSP Podcast Implementation\n",
        "\n"
      ],
      "metadata": {
        "id": "QR-w7ymYYfg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load MSP podcast"
      ],
      "metadata": {
        "id": "Vd9hXdzVb3-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1I6s62kQ0lCj"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "msp = load_dataset(\"msp-podcast\", split=\"train[:50]\")  # e.g., first 50 for test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take valency measure"
      ],
      "metadata": {
        "id": "4sTtqvfDb7lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import opensmile\n",
        "\n",
        "smile = opensmile.Smile(\n",
        "    feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "    feature_level=opensmile.FeatureLevel.Functionals\n",
        ")\n",
        "\n",
        "def extract_valence_arousal(audio_array, sr):\n",
        "    # compute features\n",
        "    features = smile.process_signal(audio_array, sr)\n",
        "    # pick valence/arousal proxies (approximation)\n",
        "    valence = features['F0semitoneFrom27.5Hz_sma3nz']  # placeholder\n",
        "    arousal = features['voicingFinalUnclipped_sma3nz']   # placeholder\n",
        "    return valence.values, arousal.values\n",
        "\n",
        "def detect_valence_boundaries(valence, arousal, sr, window_sec=2.0, threshold=0.5):\n",
        "    # compute delta over sliding window\n",
        "    delta_val = np.diff(valence)\n",
        "    delta_ar = np.diff(arousal)\n",
        "    # candidate boundary = large change\n",
        "    candidate_times = np.where(np.abs(delta_val) > threshold)[0] / sr\n",
        "    candidate_times = list(candidate_times)\n",
        "    return candidate_times"
      ],
      "metadata": {
        "id": "1QhIQcElcTr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X-mDp2arcVTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_qwen(audio, transcript, prompt):\n",
        "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\").to(model.device)\n",
        "    prompt_ids = processor.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        decoder_input_ids=prompt_ids\n",
        "    )\n",
        "    return processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "\n",
        "prompt_shift = (\n",
        "    \"Locate the first emotional shift in this segment. \"\n",
        "    \"Return JSON: {'shift_time': approximate_second, 'span': transcript_excerpt}.\"\n",
        ")\n",
        "prompt_intensity = (\n",
        "    \"Mark the beginning and end of intensity change in this segment. \"\n",
        "    \"Return JSON: {'start_time': X, 'end_time': Y, 'span': transcript_excerpt}.\"\n",
        ")\n",
        "prompt_expressive = (\n",
        "    \"Mark the emotionally expressive portion in transcript with [emote/] ... [/emote]. \"\n",
        "    \"Return only the modified transcript.\"\n",
        ")\n",
        "prompt_emotion = \"For the span tagged [emote/] ... [/emote], what is the dominant emotion? (angry, happy, sad, neutral, disgust, fear, surprise)\""
      ],
      "metadata": {
        "id": "vbW6jpoNcq88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for spk_id in set(dataset['speaker_id']):\n",
        "    # filter speaker turns\n",
        "    speaker_turns = [ex for ex in dataset if ex['speaker_id']==spk_id][:10]  # 10 turns\n",
        "    for seg_length in [30, 120]:  # 30 sec and 2 min\n",
        "        # concatenate audio + transcript until duration reached\n",
        "        audio_concat = np.concatenate([turn['audio']['array'] for turn in speaker_turns])\n",
        "        sr = speaker_turns[0]['audio']['sampling_rate']\n",
        "        if len(audio_concat)/sr > seg_length:\n",
        "            audio_concat = audio_concat[:seg_length*sr]\n",
        "\n",
        "        transcript_concat = \" \".join([turn.get('transcription') or turn.get('text') for turn in speaker_turns])\n",
        "\n",
        "        # valence/arousal analysis\n",
        "        valence, arousal = extract_valence_arousal(audio_concat, sr)\n",
        "        val_boundaries = detect_valence_boundaries(valence, arousal, sr)\n",
        "\n",
        "        # Qwen shift / intensity / expressive\n",
        "        qwen_shift = ask_qwen(audio_concat, transcript_concat, prompt_shift)\n",
        "        qwen_intensity = ask_qwen(audio_concat, transcript_concat, prompt_intensity)\n",
        "        qwen_expressive = ask_qwen(audio_concat, transcript_concat, prompt_expressive)\n",
        "        qwen_emotion = ask_qwen(audio_concat, qwen_expressive, prompt_emotion)\n",
        "\n",
        "        results.append({\n",
        "            \"speaker_id\": spk_id,\n",
        "            \"segment_duration\": f\"{seg_length}s\",\n",
        "            \"valence_boundaries\": val_boundaries,\n",
        "            \"qwen_shift\": qwen_shift,\n",
        "            \"qwen_intensity\": qwen_intensity,\n",
        "            \"expressive_span\": qwen_expressive,\n",
        "            \"emotion_label\": qwen_emotion\n",
        "        })\n",
        "\n",
        "with open(\"msp_span_detection.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"✅ Saved results to msp_span_detection.json\")"
      ],
      "metadata": {
        "id": "w6VvQ3cTck3E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}